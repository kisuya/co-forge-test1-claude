{
  "project": "project-004",
  "goal": "분석 깊이 강화 + 실데이터 파이프라인 구축 + 기반 정비. AI 분석 차별화와 실제 데이터 연동을 통해 프로토타입에서 실서비스 수준으로 도약.",
  "agent": "",
  "features": [
    {
      "id": "fix-001",
      "name": "대시보드 레거시 헤더 제거",
      "description": "dashboard/page.tsx의 레거시 헤더(54~75행, oh-my-stock + 리포트/알림/로그아웃)를 제거. 글로벌 헤더(RootLayout)가 이미 동일 기능을 제공하므로 중복. (1) <header> 블록 전체 삭제. (2) 로그아웃 기능은 글로벌 헤더 프로필 메뉴로 이전 (이미 존재하면 확인만). (3) NotificationPanel import도 글로벌 헤더에서 처리 중이면 대시보드에서 제거. 테스트: 대시보드에서 헤더가 1개만 렌더링, 로그아웃 동작 유지, 기존 테스트 통과.",
      "status": "done",
      "priority": 1,
      "test_file": "tests/frontend/test_header_dedup.py",
      "depends_on": []
    },
    {
      "id": "fix-002",
      "name": "Alembic 마이그레이션 초기 설정",
      "description": "backend/에 Alembic 환경 구성. (1) alembic init alembic으로 디렉토리 생성. (2) alembic.ini의 sqlalchemy.url을 env 변수(DATABASE_URL)에서 읽도록 수정. (3) env.py에서 Base.metadata import하여 autogenerate 지원. (4) 현재 DB 스키마 기준 초기 마이그레이션 생성 (alembic revision --autogenerate -m 'initial'). (5) create_tables() 호출을 alembic upgrade head로 대체 가능하도록 main.py 수정 (create_tables는 폴백으로 유지). 테스트: alembic upgrade head → 빈 DB에 전체 테이블 생성, alembic downgrade -1 → 롤백 성공.",
      "status": "done",
      "priority": 2,
      "test_file": "tests/backend/test_alembic_setup.py",
      "depends_on": []
    },
    {
      "id": "fix-003",
      "name": "시드 데이터 스크립트 구축",
      "description": ".forge/scripts/seed_sample_data.sh 생성. (1) PostgreSQL에 개발/검증용 샘플 데이터를 원커맨드로 삽입. (2) 포함 데이터: 2명 사용자(test@example.com/testpass123, investor@example.com/testpass123), 8개 워치리스트, 15개 시세 스냅샷, 7개 리포트(analysis JSON 포함), 12개 뉴스, 3개 브리핑, 9개 캘린더 이벤트, 6개 토론+5개 댓글, 1개 공유 리포트. (3) idempotent: 중복 실행 시 에러 없이 기존 데이터 교체. (4) docker exec로 PostgreSQL에 직접 SQL 실행. 테스트: 스크립트 실행 후 각 테이블 row count 검증, 2회 실행 시 에러 없음.",
      "status": "done",
      "priority": 3,
      "test_file": "tests/backend/test_seed_script.py",
      "depends_on": []
    },
    {
      "id": "fix-004",
      "name": "DB 커넥션 풀 안정화",
      "description": "SQLAlchemy 비동기 엔진의 커넥션 풀 설정 최적화. (1) pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800 설정. (2) 동기 엔진(create_tables용)도 동일 설정. (3) app shutdown 시 엔진 dispose() 호출. (4) health 엔드포인트에서 DB 커넥션 체크 추가 (SELECT 1). 테스트: 동시 요청 50개에서 커넥션 에러 없음, health 엔드포인트가 DB 상태 반영.",
      "status": "done",
      "priority": 4,
      "test_file": "tests/backend/test_db_pool.py",
      "depends_on": []
    },
    {
      "id": "pipe-001",
      "name": "KRX 실시세 수집 파이프라인 검증",
      "description": "krx_client.py의 PyKRX 기반 시세 수집을 실환경에서 검증·안정화. (1) fetch_current_prices: 장중(09:00~15:30)에는 당일 데이터, 장외에는 최근 거래일 데이터 조회. (2) 30개 KRX 시드 종목에 대해 일괄 수집 테스트. (3) PyKRX API 장애 시 retry 1회 + 로깅 (기존 continue 대신 structured log). (4) 수집 결과를 price_snapshots에 저장하고 Redis 캐시 갱신. (5) Celery Beat: 장중 30분 간격 스케줄 설정 (crontab minute='*/30', hour='9-15', day_of_week='mon-fri'). 테스트: mock으로 정상/에러/빈 결과 검증, 스케줄 설정 확인.",
      "status": "done",
      "priority": 5,
      "test_file": "tests/backend/test_krx_pipeline.py",
      "depends_on": ["fix-004"]
    },
    {
      "id": "pipe-002",
      "name": "미국 시세 수집 파이프라인 검증",
      "description": "us_client.py의 yfinance 기반 미국 시세 수집 안정화. (1) fetch_us_prices: 미국 장중(22:30~05:00 KST)에는 실시간, 장외에는 종가 조회. (2) 101개 US 시드 종목에 대해 일괄 수집 — yfinance rate limit 대응 (배치 처리, 100ms 딜레이 유지). (3) yfinance 장애/timeout 시 retry 1회 + 구조화 로깅. (4) Celery Beat: 미장 시간 30분 간격 (crontab minute='*/30', hour='22-23,0-5', day_of_week='mon-fri'). 테스트: mock으로 정상/에러/rate limit 검증.",
      "status": "done",
      "priority": 6,
      "test_file": "tests/backend/test_us_pipeline.py",
      "depends_on": ["fix-004"]
    },
    {
      "id": "pipe-003",
      "name": "DART 공시 수집 실연동",
      "description": "dart_client.py를 실제 DART OpenAPI와 연동. (1) API 키 환경변수(DART_API_KEY) 검증. (2) 워치리스트 종목의 최근 공시 조회 (corp_code 매핑 필요 — stocks 테이블에 dart_corp_code 컬럼 추가 또는 별도 매핑 테이블). (3) 공시 유형 필터: 주요사항보고, 실적 공시, 지분변동. (4) 중복 방지: URL 기준 unique. (5) Celery Beat: 장중 1시간 간격. (6) API 키 미설정 시 graceful skip + 경고 로그. 테스트: mock으로 공시 수집/중복 방지/키 미설정 검증.",
      "status": "done",
      "priority": 7,
      "test_file": "tests/backend/test_dart_pipeline.py",
      "depends_on": ["pipe-001"]
    },
    {
      "id": "pipe-004",
      "name": "뉴스 수집 실연동 (한국 + 미국)",
      "description": "stock_news_collector.py를 실제 뉴스 API와 연동. (1) 한국: NAVER 검색 API (client_id/secret 환경변수) — 워치리스트 종목명으로 검색, 최근 24시간 뉴스. (2) 미국: NewsAPI (api_key 환경변수) — 종목명/ticker로 검색, 무료 플랜 100건/일 제한 고려. (3) 수집된 뉴스를 news_articles 테이블에 저장 (URL unique 중복 방지). (4) Celery Beat: 1시간 간격. (5) API 키 미설정 시 graceful skip. 테스트: mock으로 한국/미국 뉴스 수집, 중복 방지, API 에러 처리.",
      "status": "done",
      "priority": 8,
      "test_file": "tests/backend/test_news_pipeline.py",
      "depends_on": ["pipe-001"]
    },
    {
      "id": "pipe-005",
      "name": "수집 모니터링 및 상태 API",
      "description": "데이터 수집 파이프라인의 상태를 모니터링하는 API. (1) GET /api/admin/pipeline-status — 각 수집기의 마지막 실행 시각, 성공/실패 여부, 수집 건수. Redis에 마지막 실행 정보 저장 (키: pipeline:{collector_name}:last_run). (2) 수집 실패 연속 3회 시 로그 레벨 ERROR로 상향. (3) 응답: {collectors: [{name, last_run_at, status, items_collected, next_scheduled_at}]}. (4) 인증 필수 (향후 admin 권한 분리 대비). 테스트: 정상 상태, 실패 상태, Redis 미연결 시 폴백.",
      "status": "done",
      "priority": 9,
      "test_file": "tests/backend/test_pipeline_status.py",
      "depends_on": ["pipe-001", "pipe-002", "pipe-004"]
    },
    {
      "id": "analysis-004",
      "name": "다층 원인 분석 프롬프트 고도화",
      "description": "llm_client.py의 분석 프롬프트를 3단계 원인 구조로 재설계. (1) 직접 원인 (Direct): 해당 종목에 직접 영향 (실적, 수주, 규제 등). (2) 간접 원인 (Indirect): 관련 산업/공급망 영향 (부품 가격, 경쟁사 실적 등). (3) 시장 환경 (Macro): 금리, 환율, 지정학 등 거시 요인. (4) 각 원인에 confidence + impact_level('critical'|'significant'|'minor') 추가. (5) analysis JSONB 구조 변경: causes[] → {direct_causes[], indirect_causes[], macro_factors[]}. (6) 기존 causes[] 호환성 유지 (마이그레이션 불필요, 새 리포트부터 적용). 테스트: 프롬프트 생성 검증, 응답 파싱, 기존 형식 호환.",
      "status": "done",
      "priority": 10,
      "test_file": "tests/backend/test_multilayer_analysis.py",
      "depends_on": []
    },
    {
      "id": "analysis-005",
      "name": "단기/중기 전망 섹션",
      "description": "AI 분석 리포트에 전망 섹션 추가. (1) 프롬프트에 전망 요청 추가: short_term_outlook(1주 이내, 2-3줄), medium_term_outlook(1개월, 2-3줄). (2) 각 전망에 sentiment('bullish'|'bearish'|'neutral') + key_catalysts[] 포함. (3) analysis JSONB에 outlook 필드 추가: {short_term: {summary, sentiment, catalysts[]}, medium_term: {summary, sentiment, catalysts[]}}. (4) 프론트엔드: 리포트 상세 페이지에 '전망' 섹션 렌더링 — bullish=초록, bearish=빨강, neutral=회색 뱃지. (5) 기존 리포트는 outlook 없으면 섹션 미표시. 테스트: 전망 생성(mock), JSONB 구조, 프론트엔드 렌더링, 기존 리포트 호환.",
      "status": "done",
      "priority": 11,
      "test_file": "tests/backend/test_outlook_analysis.py",
      "depends_on": ["analysis-004"]
    },
    {
      "id": "analysis-006",
      "name": "섹터 연쇄 영향 분석",
      "description": "급변동 종목과 동일 섹터/공급망 종목의 연쇄 영향 분석. (1) stocks 테이블의 sector 필드 활용 — 동일 섹터 종목 조회. (2) 급변동 발생 시 동일 섹터 종목의 당일 변동률도 함께 수집. (3) AI 프롬프트에 섹터 컨텍스트 추가: '같은 섹터({sector}) 종목 {N}개 중 {M}개가 동반 하락/상승'. (4) analysis JSONB에 sector_impact 필드: {sector, related_stocks: [{name, code, change_pct}], correlation_note}. (5) 프론트엔드: 리포트에 '섹터 영향' 카드 — 관련 종목 변동률 시각화 (가로 바 차트 또는 리스트). (6) 섹터 정보 없는 종목: 섹션 미표시. 테스트: 섹터 연쇄 분석 생성, 관련 종목 조회, 프론트엔드 렌더링.",
      "status": "done",
      "priority": 12,
      "test_file": "tests/backend/test_sector_analysis.py",
      "depends_on": ["analysis-004", "pipe-001"]
    },
    {
      "id": "analysis-007",
      "name": "유사 케이스 이후 주가 추이 비교",
      "description": "유사 과거 사례 발생 이후 실제 주가가 어떻게 변했는지 추적 데이터 제공. (1) similar_cases의 각 사례에 aftermath 필드 추가: {after_1w_pct, after_1m_pct, recovery_days(원래 가격 회복까지 소요일, null이면 미회복)}. (2) price_snapshots에서 해당 날짜 이후 1주/1개월 시점의 가격 변동률 계산. (3) 데이터 부족 시 (수집 이전 사례) aftermath=null + '데이터 부족' 안내. (4) 프론트엔드: 유사 사례 카드에 '이후 추이' 표시 — '1주 후 +3.2%, 1개월 후 +8.5%' 또는 '회복까지 12일 소요'. (5) 현재 사례와 과거 사례의 유사점/차이점 AI 분석 1줄 추가. 테스트: aftermath 계산 정확성, 데이터 부족 처리, 프론트엔드 렌더링.",
      "status": "done",
      "priority": 13,
      "test_file": "tests/backend/test_case_aftermath.py",
      "depends_on": ["analysis-004"]
    },
    {
      "id": "analysis-008",
      "name": "뉴스 감성 분석 트렌드",
      "description": "종목별 뉴스 감성을 시간에 따라 추적하여 트렌드 시각화. (1) news_articles에 sentiment 컬럼 추가 ('positive'|'negative'|'neutral', float score -1.0~1.0). (2) news_summarizer에서 요약 시 감성도 함께 분석 (기존 importance와 동시 처리, 추가 API 호출 없음). (3) GET /api/stocks/{stock_id}/sentiment — 최근 30일 일별 평균 감성 점수 반환: [{date, avg_score, article_count}]. (4) 프론트엔드: 종목 상세 페이지에 '뉴스 감성 트렌드' 차트 (간단한 라인 차트, CSS/SVG 기반 — 외부 차트 라이브러리 불필요). (5) 뉴스 부족 시(일 1건 미만) 해당 날짜 건너뜀. 테스트: 감성 분석 파싱, 일별 집계, API 응답, 차트 렌더링.",
      "status": "done",
      "priority": 14,
      "test_file": "tests/backend/test_sentiment_trend.py",
      "depends_on": ["pipe-004"]
    },
    {
      "id": "analysis-009",
      "name": "분석 리포트 UI 고도화",
      "description": "리포트 상세 페이지를 다층 분석·전망·섹터 영향·유사 케이스 추이에 맞게 재구성. (1) 원인 분석 섹션: 3단계(직접/간접/거시) 탭 또는 아코디언으로 구분. 각 원인에 impact_level 뱃지(critical=빨강, significant=주황, minor=회색). (2) 전망 섹션: short_term/medium_term 카드 2개. sentiment 뱃지(bullish=초록, bearish=빨강, neutral=회색). key_catalysts bullet list. (3) 섹터 영향 섹션: 관련 종목 변동률 리스트. (4) 유사 사례 섹션: aftermath 표시(1주/1개월 추이). (5) 기존 리포트(새 필드 없는)는 기존 UI 유지 (graceful fallback). 테스트: 각 섹션 렌더링, 기존 리포트 호환, 빈 섹션 미표시.",
      "status": "done",
      "priority": 15,
      "test_file": "tests/frontend/test_report_enhanced.py",
      "depends_on": ["analysis-005", "analysis-006", "analysis-007"]
    },
    {
      "id": "pipe-006",
      "name": "급변동 감지→리포트 생성 end-to-end 파이프라인",
      "description": "시세 수집 → 급변동 감지 → 뉴스/공시 수집 → AI 분석 → 리포트 저장 → 알림의 전체 파이프라인을 실동작으로 검증. (1) price_collector가 변동률 threshold 초과 감지 시 Report(status=pending) 생성. (2) Celery chain: collect_prices → detect_movements → collect_news → analyze → complete_report. (3) 각 단계의 실패 시 다음 단계로 전파되지 않도록 에러 격리. (4) 리포트 생성 완료 시 WebPush 알림 전송 (push_worker). (5) 전체 파이프라인 소요 시간 로깅. 테스트: mock으로 전체 체인 실행, 중간 실패 시 격리 검증, 소요 시간 로깅.",
      "status": "done",
      "priority": 16,
      "test_file": "tests/backend/test_e2e_pipeline.py",
      "depends_on": ["pipe-001", "pipe-003", "pipe-004", "analysis-004"]
    }
  ]
}
